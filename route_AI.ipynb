{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "route_AI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UekZ4NrLjyRK"
      },
      "source": [
        "  \n",
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"ai_fp_route_optimization.ipynb\n",
        "#\n",
        "# Automatically generated by Colaboratory.\n",
        "#\n",
        "# Original file is located at\n",
        "#     https://colab.research.google.com/drive/1mkd8zWzcmFTUBYzIR-kt1emzqieAt2Uz\n",
        "#\n",
        "# import 函式庫\n",
        "# \"\"\"\n",
        "#\n",
        "import math\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import load_model\n",
        "#\n",
        "ALL_ROUTE = ['R', 'E', '$', '?','M','T']\n",
        "#\n",
        "class Route_ai:\n",
        "#     \"\"\"docstring for Route_ai.\"\"\"\n",
        "#\n",
        "    def __init__(self):\n",
        "        self.model=load_model('AI_FP_ROUTE.h5')\n",
        "#\n",
        "    def encode_single(self, value, category):\n",
        "      np_array = np.array([[value]])\n",
        "      encoder = OneHotEncoder(categories=[category], sparse=False)\n",
        "      onehot_encoded = encoder.fit_transform(np_array)\n",
        "      collapsed = np.sum(onehot_encoded, axis=0)\n",
        "#       # inverse = encoder.inverse_transform(collapsed[np.newaxis, ...])\n",
        "#       # print(np.array_equal(np_array, inverse))\n",
        "      return collapsed\n",
        "#\n",
        "    def encode_route(self,route):\n",
        "#       \"\"\"\n",
        "#       Encode the chosen character into a one-hot vector of length ALL_CHARACTERS\n",
        "#       \"\"\"\n",
        "      ALL_ROUTE.sort()\n",
        "      return self.encode_single(route,ALL_ROUTE)\n",
        "#\n",
        "    def encode_sample_with_loop(self, sample):\n",
        "#       \"\"\"\n",
        "#       Encode a single sample into a 1D vector\n",
        "#       [{\"cards\": 10, \"relics\": 1, \"ascension\": 20, \"character\": \"IRONCLAD\", \"floor\": 0, \"potions\": 0, \"path\": \"M\", \"max_hp\": 82, \"current_hp\": 82, \"gold\": 99, \"value\": 2884.076040777472, \"upgrade_cards\": 0, \"curse_cards\": 1}\n",
        "#       \"\"\"\n",
        "      route=self.encode_route(sample['path'])\n",
        "#      # print(character)\n",
        "      num_data = np.array([sample['cards'],sample['relics'],sample['ascension'],sample['floor'],sample['potions'],sample['max_hp'],sample['current_hp'],sample['gold'],sample['upgrade_cards'],sample['curse_cards']])\n",
        "      x=np.concatenate((route, num_data))\n",
        "#       #print(len(x))\n",
        "      return  x\n",
        "      #return num_data\n",
        "#\n",
        "    def preprocess_with_loop(self, data):\n",
        "      preprocess_list=[]\n",
        "      y=[]\n",
        "      for i,sample in enumerate(data):\n",
        "        if sample['path']!= None and sample['path']!='B':\n",
        "          preprocess_list.append(self.encode_sample_with_loop(sample))\n",
        "          y.append(sample['value'])\n",
        "      X=np.vstack(preprocess_list)\n",
        "      #print(len(X))\n",
        "      Y=np.array(y,dtype='float64')\n",
        "#       #print(len(Y))\n",
        "      return X,Y\n",
        "#\n",
        "    def scale_X(self,X_data):\n",
        "#       \"\"\"\n",
        "#       Used with one hot encoded model\n",
        "#       \"\"\"\n",
        "      X_copy = np.copy(X_data)\n",
        "      X_copy=np.array(X_copy).reshape(1,-1)\n",
        "      max_abs_scaler = MaxAbsScaler()\n",
        "      X_maxabs = max_abs_scaler.fit_transform(X_copy)\n",
        "#     #  with open('input_scales.json', 'w') as out_file:\n",
        "#      #   json.dump(max_abs_scaler.scale_.tolist(), out_file)\n",
        "      \n",
        "      return X_maxabs\n",
        "#\n",
        "    def scale_Y(self, Y_data):\n",
        "      Y_copy = np.copy(Y_data)\n",
        "#\n",
        "#       # Scale Y\n",
        "      Y_copy /= 10\n",
        "#\n",
        "#       # To allow healing (negative damage), uncomment `Y[Y < -1] = -1` and comment out `Y[Y < 0] = 0`\n",
        "#       #Y_copy[Y_copy < -1] = -1 # Healing (negative damage)\n",
        "#       # Y_copy[Y_copy < 0] = 0 # No healing\n",
        "#\n",
        "#       # Cap damage taken at 100\n",
        "      Y_copy[Y_copy > 800] = 800\n",
        "      return Y_copy\n",
        "#\n",
        "    def predict(self, data):\n",
        "      x=self.model.predict(self.scale_X(self.encode_sample_with_loop(data)))\n",
        "      return x\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwD4bT-Bmw-r",
        "outputId": "c43fd1ec-98ff-4099-c2a3-60823a9759e9"
      },
      "source": [
        "fuck=Route_ai()\n",
        "shit=dict()\n",
        "shit={\n",
        "    \"cards\": 13,\n",
        "    \"upgrade_cards\": 0,\n",
        "    \"curse_cards\": 0,\n",
        "    \"relics\": 1,\n",
        "    \"ascension\": 0,\n",
        "    \"floor\": 4,\n",
        "    \"potions\": 1,\n",
        "    \"max_hp\": 80,\n",
        "    \"current_hp\": 71,\n",
        "    \"gold\": 152,\n",
        "    \"path\": \"E\"\n",
        "}\n",
        "print(fuck.predict(shit))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[529.3298]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}